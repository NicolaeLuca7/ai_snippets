{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aXWFjmHDpuOq",
      "metadata": {
        "id": "aXWFjmHDpuOq"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20096e79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20096e79",
        "outputId": "693db097-2663-4b88-9d64-3ceef3eb4e07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37a57e57",
      "metadata": {
        "id": "37a57e57"
      },
      "outputs": [],
      "source": [
        "%pip install huggingface_hub diffusers datasets transformers accelerate bitsandbytes tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "904015710ceb49f9",
      "metadata": {
        "id": "904015710ceb49f9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers.utils import make_image_grid\n",
        "from torchvision.transforms.functional import pil_to_tensor, to_pil_image\n",
        "from PIL import Image\n",
        "import requests\n",
        "from diffusers import DiffusionPipeline\n",
        "import bitsandbytes as bnb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b571rFbZ_a8H",
      "metadata": {
        "id": "b571rFbZ_a8H"
      },
      "outputs": [],
      "source": [
        "seed_value = 42\n",
        "torch.manual_seed(seed_value)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed(seed_value)\n",
        "  torch.cuda.manual_seed_all(seed_value)\n",
        "np.random.seed(seed_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6X18oLOf1mir",
      "metadata": {
        "id": "6X18oLOf1mir"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Replace 'your_token_here' with your actual Hugging Face API token\n",
        "login(token='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')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40d16575e573142c",
      "metadata": {
        "id": "40d16575e573142c"
      },
      "outputs": [],
      "source": [
        "learning_rate = 3e-05\n",
        "resolution = 224# <256\n",
        "max_train_steps = 7000#5000\n",
        "train_batch_size = 16#16 8\n",
        "accumulation_steps= 2\n",
        "lr_updates, gamma = 35, 0.97\n",
        "score_checks = 5\n",
        "score_steps = max_train_steps // score_checks\n",
        "test_prints = 15\n",
        "test_steps = max_train_steps // test_prints\n",
        "\n",
        "base_model_name =\"lambdalabs/miniSD-diffusers\"\n",
        "\n",
        "# Extract the individual components\n",
        "pipe = DiffusionPipeline.from_pretrained(base_model_name,torch_dtype=torch.float32,\n",
        "                                         safety_checker = None,\n",
        "                                        requires_safety_checker = False)\n",
        "pipe.to('cuda')\n",
        "vae = pipe.vae\n",
        "text_encoder = pipe.text_encoder\n",
        "tokenizer = pipe.tokenizer\n",
        "unet = pipe.unet\n",
        "noise_scheduler = pipe.scheduler\n",
        "\n",
        "# Freeze vae and text_encoder and set unet to trainable\n",
        "\n",
        "train_vae=False\n",
        "train_unet=True\n",
        "\n",
        "if train_vae==True :\n",
        "  vae.requires_grad_(True)\n",
        "  vae.train()\n",
        "else:\n",
        "  vae.requires_grad_(False)\n",
        "\n",
        "if train_unet==True :\n",
        "  unet.requires_grad_(True)\n",
        "  unet.train()\n",
        "else:\n",
        "  unet.requires_grad_(False)\n",
        "\n",
        "text_encoder.requires_grad_(False)\n",
        "\n",
        "unet_optimizer = bnb.optim.Adam8bit(unet.parameters(), lr=learning_rate, betas=(0.9, 0.98))\n",
        "vae_optimizer=bnb.optim.Adam8bit(vae.parameters(), lr=learning_rate)\n",
        "unet_lr_scheduler = torch.optim.lr_scheduler.StepLR(unet_optimizer, step_size=max_train_steps//(accumulation_steps * lr_updates), gamma=gamma)\n",
        "vae_lr_scheduler = torch.optim.lr_scheduler.StepLR(vae_optimizer, step_size=max_train_steps//(accumulation_steps * lr_updates), gamma=gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58ea4abc",
      "metadata": {
        "id": "58ea4abc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms import functional as TF\n",
        "\n",
        "class RandomApply(T.RandomApply):\n",
        "    def __init__(self, transforms, p=0.5):\n",
        "        super().__init__(transforms, p=p)\n",
        "\n",
        "class AddGaussianNoise(torch.nn.Module):\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        super().__init__()\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def forward(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n",
        "\n",
        "class Cutout(torch.nn.Module):\n",
        "    def __init__(self, size=16):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, img):\n",
        "        h, w = img.size(1), img.size(2)\n",
        "        mask = torch.ones_like(img)\n",
        "        y = torch.randint(h, (1,))\n",
        "        x = torch.randint(w, (1,))\n",
        "\n",
        "        y1 = torch.clamp(y - self.size // 2, 0, h)\n",
        "        y2 = torch.clamp(y + self.size // 2, 0, h)\n",
        "        x1 = torch.clamp(x - self.size // 2, 0, w)\n",
        "        x2 = torch.clamp(x + self.size // 2, 0, w)\n",
        "\n",
        "        mask[:, y1:y2, x1:x2] = 0\n",
        "        return img * mask\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + f'(size={self.size})'\n",
        "\n",
        "\n",
        "\n",
        "# Preprocessing the datasets.\n",
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "        transforms.ToTensor(),\n",
        "\n",
        "        # ----\n",
        "        # TODO 3.5 (very low priority): You might add additional augmentation\n",
        "        transforms.CenterCrop(resolution),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        RandomApply([T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)], p=0.5),\n",
        "        RandomApply([AddGaussianNoise(mean=0., std=0.1)], p=0.5),\n",
        "        RandomApply([T.RandomRotation(degrees=10)], p=0.5),\n",
        "        RandomApply([Cutout(size=16)], p=0.5),\n",
        "\n",
        "        #----\n",
        "        transforms.Normalize([0.5], [0.5]),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hc588UZh_ona",
      "metadata": {
        "id": "Hc588UZh_ona"
      },
      "outputs": [],
      "source": [
        "#All labels for object detector except zebra and giraffe\n",
        "\n",
        "labels=[\n",
        "    \"zebra\",\n",
        "    \"giraffe\",\n",
        "    \"person\",\n",
        "    \"bicycle\",\n",
        "    \"car\",\n",
        "    \"motorcycle\",\n",
        "    \"airplane\",\n",
        "    \"bus\",\n",
        "    \"train\",\n",
        "    \"truck\",\n",
        "    \"boat\",\n",
        "    \"traffic light\",\n",
        "    \"fire hydrant\",\n",
        "    \"stop sign\",\n",
        "    \"parking meter\",\n",
        "    \"bench\",\n",
        "    \"bird\",\n",
        "    \"cat\",\n",
        "    \"dog\",\n",
        "    \"horse\",\n",
        "    \"sheep\",\n",
        "    \"cow\",\n",
        "    \"elephant\",\n",
        "    \"bear\",\n",
        "    \"backpack\",\n",
        "    \"umbrella\",\n",
        "    \"handbag\",\n",
        "    \"tie\",\n",
        "    \"suitcase\",\n",
        "    \"frisbee\",\n",
        "    \"skis\",\n",
        "    \"snowboard\",\n",
        "    \"sports ball\",\n",
        "    \"kite\",\n",
        "    \"baseball bat\",\n",
        "    \"baseball glove\",\n",
        "    \"skateboard\",\n",
        "    \"surfboard\",\n",
        "    \"tennis racket\",\n",
        "    \"bottle\",\n",
        "    \"wine glass\",\n",
        "    \"cup\",\n",
        "    \"fork\",\n",
        "    \"knife\",\n",
        "    \"spoon\",\n",
        "    \"bowl\",\n",
        "    \"banana\",\n",
        "    \"apple\",\n",
        "    \"sandwich\",\n",
        "    \"orange\",\n",
        "    \"broccoli\",\n",
        "    \"carrot\",\n",
        "    \"hot dog\",\n",
        "    \"pizza\",\n",
        "    \"donut\",\n",
        "    \"cake\",\n",
        "    \"chair\",\n",
        "    \"couch\",\n",
        "    \"potted plant\",\n",
        "    \"bed\",\n",
        "    \"dining table\",\n",
        "    \"toilet\",\n",
        "    \"TV\",\n",
        "    \"laptop\",\n",
        "    \"mouse\",\n",
        "    \"remote\",\n",
        "    \"keyboard\",\n",
        "    \"cell phone\",\n",
        "    \"microwave\",\n",
        "    \"oven\",\n",
        "    \"toaster\",\n",
        "    \"sink\",\n",
        "    \"refrigerator\",\n",
        "    \"book\",\n",
        "    \"clock\",\n",
        "    \"vase\",\n",
        "    \"scissors\",\n",
        "    \"teddy bear\",\n",
        "    \"hair drier\",\n",
        "    \"toothbrush\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vdfaqIWL0o-Z",
      "metadata": {
        "id": "vdfaqIWL0o-Z"
      },
      "outputs": [],
      "source": [
        "dataset=load_dataset('Romania1/cv_dataset', trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10DS7HOx3FXa",
      "metadata": {
        "id": "10DS7HOx3FXa"
      },
      "outputs": [],
      "source": [
        "dataset=dataset['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b9c26fe8978069",
      "metadata": {
        "id": "9b9c26fe8978069"
      },
      "outputs": [],
      "source": [
        "# convert dataset to a loader that could be feed during training\n",
        "def tokenize_captions(examples, is_train=True):\n",
        "    captions = examples['text']\n",
        "    inputs = tokenizer(\n",
        "        captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    return inputs.input_ids\n",
        "\n",
        "def preprocess_train(examples):\n",
        "    images = [image.convert(\"RGB\") for image in examples['image']]\n",
        "    examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n",
        "    examples[\"input_ids\"] = tokenize_captions(examples)\n",
        "    return examples\n",
        "\n",
        "\n",
        "train_dataset = dataset.with_transform(preprocess_train)\n",
        "\n",
        "def collate_fn(examples):\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
        "    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    batch_size=train_batch_size,\n",
        "    num_workers=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8vNsOCyCKqai",
      "metadata": {
        "collapsed": true,
        "id": "8vNsOCyCKqai"
      },
      "outputs": [],
      "source": [
        "# Training itself\n",
        "device = 'cuda'\n",
        "weight_dtype = torch.float32 #torch.float16\n",
        "\n",
        "# Move text_encode and vae to gpu and cast to weight_dtype\n",
        "text_encoder.to(device, dtype=weight_dtype)\n",
        "vae.to(device, dtype=weight_dtype)\n",
        "unet.to(device, dtype=weight_dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MhQDc_5vC-gG",
      "metadata": {
        "id": "MhQDc_5vC-gG"
      },
      "outputs": [],
      "source": [
        "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
        "\n",
        "\n",
        "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
        "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
        "model.to(device)\n",
        "\n",
        "def detect(image):\n",
        "    inputs = image_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    outputs = model(**inputs)\n",
        "    target_sizes = torch.tensor([image.size[::-1]])\n",
        "    results = image_processor.post_process_object_detection(outputs, threshold=0.6, target_sizes=target_sizes)[0]\n",
        "    objects = [model.config.id2label[idx.item()] for idx in results['labels']]\n",
        "    return objects\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J0df0V8RC1lC",
      "metadata": {
        "id": "J0df0V8RC1lC"
      },
      "outputs": [],
      "source": [
        "def get_score(objects, label):\n",
        "    found_objects = set(objects).intersection(set(labels))\n",
        "    if label not in found_objects:\n",
        "      return 0\n",
        "\n",
        "    if label == 'zebra':\n",
        "      if 'giraffe' in found_objects:\n",
        "        return 0\n",
        "      else:\n",
        "        return 2\n",
        "\n",
        "    if label == 'giraffe':\n",
        "      if 'zebra' in found_objects:\n",
        "        return 0\n",
        "      else:\n",
        "        return 2\n",
        "\n",
        "    return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6A3RV9V1FQFu",
      "metadata": {
        "id": "6A3RV9V1FQFu"
      },
      "outputs": [],
      "source": [
        "def generate(pipe, prompt):\n",
        "    image = pipe(\n",
        "        prompt=prompt, num_inference_steps=50, guidance_scale=8.5,\n",
        "        generator=torch.Generator(device=device).manual_seed(seed_value)\n",
        "    ).images[0]\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l-fw4U6aDEZD",
      "metadata": {
        "id": "l-fw4U6aDEZD"
      },
      "outputs": [],
      "source": [
        "test_prompts = [\n",
        "    \"A zebra stands proudly amidst vibrant tropical foliage and a cascading waterfall.\",\n",
        "    \"A zebra playfully trots through a snowy winter landscape, with snow-covered trees and a cozy cabin in the background.\",\n",
        "    \"A giraffe with a sleek, holographic collar stands majestically in a bustling, neon-lit futuristic cityscape.\",\n",
        "    \"A giraffe strolls along a sandy beach at sunset, with the waves gently lapping at its hooves.\",\n",
        "    \"A colorful parrot perched on a branch against a blue sky. The parrot has bright red and green feathers, with a yellow beak.\",\n",
        "    \"A car driving through a neon-lit city at night, with reflections of vibrant lights bouncing off its polished surface.\",\n",
        "    \"A red umbrella stands out in a rainy, grey cityscape.\",\n",
        "    \"A rugged backpack with patches by a mossy tree in a sunlit forest.\",\n",
        "    \"In a snowy forest, a cozy bear stands under snow-covered trees, enjoying the gentle snowfall.\",\n",
        "    \"A modern computer mouse with a sleek design. It has a matte black finish with a glowing blue scroll wheel. The mouse is placed on a white desk surface.\"\n",
        "]\n",
        "\n",
        "test_labels = [\n",
        "    \"giraffe\",\n",
        "    \"giraffe\",\n",
        "    \"zebra\",\n",
        "    \"zebra\",\n",
        "    \"bird\",\n",
        "    \"car\",\n",
        "    \"umbrella\",\n",
        "    \"backpack\",\n",
        "    \"bear\",\n",
        "    \"mouse\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PucXjpxwJxYR",
      "metadata": {
        "id": "PucXjpxwJxYR"
      },
      "outputs": [],
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import os\n",
        "import torch\n",
        "\n",
        "def save_stable_diffusion_pipeline(pipeline_obj, directory):\n",
        "    \"\"\"Save a Stable Diffusion pipeline to a local directory.\"\"\"\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    pipeline_obj.save_pretrained(directory)\n",
        "    print(f\"Pipeline saved to {directory}\")\n",
        "\n",
        "def load_and_optimize_pipeline(model_path, weight_dtype=torch.float32):\n",
        "    # Load the pipeline\n",
        "    pipe = DiffusionPipeline.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=weight_dtype,\n",
        "    )\n",
        "\n",
        "    # Ensure it's on the correct device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    # Enable optimizations\n",
        "    if device == \"cuda\":\n",
        "        pipe.enable_attention_slicing()\n",
        "    pipe.enable_vae_slicing()\n",
        "\n",
        "    # Warm-up run\n",
        "    _ = pipe(\"Warm-up prompt\", num_inference_steps=1)\n",
        "\n",
        "    return pipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JTB0cdJQccu2",
      "metadata": {
        "id": "JTB0cdJQccu2"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def image_grid(imgs, rows=2, cols=2):\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "435b54c87c5a20b9",
      "metadata": {
        "id": "435b54c87c5a20b9"
      },
      "outputs": [],
      "source": [
        "num_train_epochs = math.ceil(max_train_steps * train_batch_size / len(train_dataset))\n",
        "print(\"***** Running training *****\")\n",
        "print(f\"  Num examples = {len(train_dataset)}\")\n",
        "print(f\"  Num Epochs = {num_train_epochs}\")\n",
        "print(f\"  Instantaneous batch size per device = {train_batch_size}\")\n",
        "print(f\"  Total optimization steps = {max_train_steps}\")\n",
        "\n",
        "global_step = 0\n",
        "initial_global_step = 0\n",
        "\n",
        "progress_bar = tqdm(\n",
        "    range(0, max_train_steps),\n",
        "    initial=initial_global_step,\n",
        "    desc=\"Steps\",\n",
        ")\n",
        "\n",
        "losses = []\n",
        "EPOCH=[]\n",
        "LOSS=[]\n",
        "best_score, best_loss = -1, -1\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    tloss=0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Convert images to latent space\n",
        "        latents = vae.encode(batch[\"pixel_values\"].to(weight_dtype).to(device)).latent_dist.sample()\n",
        "        latents = latents * vae.config.scaling_factor\n",
        "\n",
        "        # Sample noise that we'll add to the latents\n",
        "        noise = torch.randn_like(latents)\n",
        "        batch_size = latents.shape[0]\n",
        "        # Sample a random timestep for each image\n",
        "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (batch_size,), device=latents.device)\n",
        "        timesteps = timesteps.long()\n",
        "\n",
        "        # Add noise to the latents according to the noise magnitude at each timestep\n",
        "        # (this is the forward diffusion process)\n",
        "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "        # Get the text embedding for conditioning\n",
        "        encoder_hidden_states = text_encoder(batch[\"input_ids\"].to('cuda'), return_dict=False)[0]\n",
        "\n",
        "        # Predict the noise residual and compute loss\n",
        "        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]\n",
        "        loss = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
        "\n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)\n",
        "\n",
        "        if (step+1)%accumulation_steps==0:\n",
        "          if train_unet:\n",
        "            for param in unet.parameters():\n",
        "               param.grad/=accumulation_steps\n",
        "\n",
        "            unet_optimizer.step()\n",
        "\n",
        "            unet_optimizer.zero_grad()\n",
        "            unet_lr_scheduler.step()\n",
        "\n",
        "          if train_vae:\n",
        "            for param in vae.parameters():\n",
        "               param.grad/=accumulation_steps\n",
        "\n",
        "            vae_optimizer.step()\n",
        "\n",
        "            vae_optimizer.zero_grad()\n",
        "            vae_lr_scheduler.step()\n",
        "        ###############################################################\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        progress_bar.update(1)\n",
        "        global_step += 1\n",
        "\n",
        "        #Testing the score of the model\n",
        "        # if global_step % score_steps == 0:\n",
        "        #   if train_unet: unet.eval()\n",
        "        #   if train_vae: vae.eval()\n",
        "\n",
        "        #   total_score = 0\n",
        "        #   generated_images = []\n",
        "        #   for prompt, label in zip(test_prompts, test_labels):\n",
        "        #     image = generate(pipe, prompt)\n",
        "        #     generated_images.append(image)\n",
        "        #     # objects = detect(image)\n",
        "        #     # score = get_score(objects, label)\n",
        "        #     # print(f'Score: {score}, caut {label} si am gasit: {set(objects)}')\n",
        "        #     # total_score += score\n",
        "\n",
        "        #   # print(f'SCORE: {total_score}')\n",
        "        #   # best_model = False\n",
        "        #   # avg_loss = np.mean(losses[-20:])\n",
        "        #   # if total_score >= best_score:\n",
        "        #   #   best_score = total_score\n",
        "        #   #   best_loss = avg_loss\n",
        "        #   #   # save_stable_diffusion_pipeline(pipe, \"./my_stable_diffusion_pipeline\")\n",
        "        #   #   pipe.push_to_hub(repo_url)\n",
        "        #   #   print('Saving best model!')\n",
        "        #   #   best_model = True\n",
        "\n",
        "        #   # if best_model:\n",
        "        #   display(image_grid(generated_images, 2, 5))\n",
        "\n",
        "        #   if train_unet: unet.train()\n",
        "        #   if train_vae: vae.train()\n",
        "\n",
        "        # if global_step%test_steps==0:\n",
        "        #   unet.eval()\n",
        "        #   image = pipe(\"Beautiful giraffe running in savana\", width=resolution, height=resolution).images[0]\n",
        "        #   display(image.resize((512, 512)))\n",
        "        #   image = pipe(\"Beautiful zebra running in savana\", width=resolution, height=resolution).images[0]\n",
        "        #   display(image.resize((512, 512)))\n",
        "        #   unet.train()\n",
        "\n",
        "        progress_bar.set_postfix(average_loss=np.mean(losses[-20:]), step=global_step)\n",
        "        tloss+=np.mean(losses[-20:])\n",
        "        if global_step >= max_train_steps:\n",
        "            break\n",
        "    LOSS.append(tloss)\n",
        "    EPOCH.append(epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3vvctuDTKAZt",
      "metadata": {
        "id": "3vvctuDTKAZt"
      },
      "outputs": [],
      "source": [
        "plt.plot(EPOCH,LOSS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H6_FJjgV59p0",
      "metadata": {
        "id": "H6_FJjgV59p0"
      },
      "outputs": [],
      "source": [
        "image = pipe(\"Giraffe in snowy city\", width=resolution, height=resolution).images[0]\n",
        "display(image.resize((512, 512)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f346719b561e2a04",
      "metadata": {
        "id": "f346719b561e2a04"
      },
      "source": [
        "As we see, it's starting to do it correctly, but there is definetely some room for improvement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd10048f78b39b4b",
      "metadata": {
        "id": "fd10048f78b39b4b"
      },
      "source": [
        "# Submission\n",
        "To determine how well the model performs, we'll evaluate it using another notebook. For this reason, you need to upload the copy of trained pipeline to Hugging Face.\n",
        "\n",
        "1. Register the team at [Hugging Face](https://huggingface.co) or login if you have account alrady.\n",
        "2. Obtain an access token with write rights from [Hugging Face Tokens](https://huggingface.co/settings/tokens).\n",
        "3. In the code below, replace account name with the one you registered and model name with any name you find approprate.\n",
        "4. Enter the access token.\n",
        "\n",
        "Use the [evaluation notebook](https://colab.research.google.com/drive/12eRsJK5AUDoKZOFQo60pzMLdmSJZhl3E) to check the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc07e41d2c8fd603",
      "metadata": {
        "id": "bc07e41d2c8fd603"
      },
      "outputs": [],
      "source": [
        "new_pipeline = DiffusionPipeline.from_pretrained(\n",
        "    base_model_name,\n",
        "    vae=vae,\n",
        "    unet=unet,\n",
        "    text_encoder=text_encoder\n",
        ")\n",
        "new_pipeline.push_to_hub(\"Romania1/cv_model\", token='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')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}